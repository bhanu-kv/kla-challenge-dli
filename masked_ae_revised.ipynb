{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss() \n\u001b[0;32m    112\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m model_trained\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     82\u001b[0m degraded, mask, gt \u001b[38;5;241m=\u001b[39m degraded\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device), gt\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdegraded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, gt)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mMaskedAutoencoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     63\u001b[0m encoded_unmasked_upsampled \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[0;32m     64\u001b[0m     encoded_unmasked, size\u001b[38;5;241m=\u001b[39m(masked_input\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), masked_input\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     67\u001b[0m combined_decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((encoded_unmasked_upsampled, masked_input), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_decoder_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m decoded \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(decoded, size\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m output \u001b[38;5;241m=\u001b[39m decoded \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m+\u001b[39m x \u001b[38;5;241m*\u001b[39m mask\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py:953\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    948\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    949\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    951\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class DenoisingDataset(Dataset):\n",
    "    def __init__(self, degraded_dir, mask_dir, gt_dir, transform=None):\n",
    "        self.degraded_dir = degraded_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(degraded_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        degraded_img = Image.open(os.path.join(self.degraded_dir, self.images[idx])).convert('RGB')\n",
    "        mask_img_name = self.images[idx].replace('.png', '_mask.png')\n",
    "        mask_img = Image.open(os.path.join(self.mask_dir, mask_img_name)).convert('L')\n",
    "        gt_img = Image.open(os.path.join(self.gt_dir, self.images[idx])).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            degraded_img = self.transform(degraded_img)\n",
    "            mask_img = self.transform(mask_img)\n",
    "            gt_img = self.transform(gt_img)\n",
    "        \n",
    "        return degraded_img, mask_img, gt_img\n",
    "\n",
    "\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(259, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        unmasked_input = x * (1 - mask)\n",
    "        encoded_unmasked = self.encoder(unmasked_input)\n",
    "        \n",
    "        masked_input = x * mask\n",
    "        encoded_unmasked_upsampled = nn.functional.interpolate(\n",
    "            encoded_unmasked, size=(masked_input.size(2), masked_input.size(3)), mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "        combined_decoder_input = torch.cat((encoded_unmasked_upsampled, masked_input), dim=1)\n",
    "        decoded = self.decoder(combined_decoder_input)\n",
    "\n",
    "\n",
    "        decoded = nn.functional.interpolate(decoded, size=x.size()[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        output = decoded * (1 - mask) + x * mask\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def train_autoencoder(model, dataloader, criterion, optimizer, num_epochs=1):\n",
    "    for epoch in range(num_epochs):\n",
    "        for degraded, mask, gt in dataloader:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            degraded, mask, gt = degraded.to(device), mask.to(device), gt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(degraded, mask)\n",
    "            loss = criterion(outputs, gt)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    degraded_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Train\\Degraded_image\\broken_small\"\n",
    "    mask_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Train\\Defect_mask\\broken_small\"\n",
    "    gt_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Train\\GT_clean_image\\broken_small\"\n",
    "    \n",
    "    dataset = DenoisingDataset(degraded_dir, mask_dir, gt_dir, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MaskedAutoencoder().to(device) \n",
    "    criterion = nn.MSELoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model_trained= train_autoencoder(model, dataloader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.0506\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class DenoisingDataset(Dataset):\n",
    "    def __init__(self, degraded_dir, mask_dir, gt_dir, transform=None):\n",
    "        self.degraded_dir = degraded_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(degraded_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        degraded_img = Image.open(os.path.join(self.degraded_dir, self.images[idx])).convert('RGB')\n",
    "        mask_img_name = self.images[idx].replace('.png', '_mask.png')\n",
    "        mask_img = Image.open(os.path.join(self.mask_dir, mask_img_name)).convert('L')\n",
    "        gt_img = Image.open(os.path.join(self.gt_dir, self.images[idx])).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            degraded_img = self.transform(degraded_img)\n",
    "            mask_img = self.transform(mask_img)\n",
    "            gt_img = self.transform(gt_img)\n",
    "        \n",
    "        return degraded_img, mask_img, gt_img\n",
    "\n",
    "\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(259, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        unmasked_input = x * (1 - mask)\n",
    "        encoded_unmasked = self.encoder(unmasked_input)\n",
    "        \n",
    "        masked_input = x * mask\n",
    "        encoded_unmasked_upsampled = nn.functional.interpolate(\n",
    "            encoded_unmasked, size=(masked_input.size(2), masked_input.size(3)), mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "        combined_decoder_input = torch.cat((encoded_unmasked_upsampled, masked_input), dim=1)\n",
    "        decoded = self.decoder(combined_decoder_input)\n",
    "\n",
    "        decoded = nn.functional.interpolate(decoded, size=x.size()[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        output = decoded * (1 - mask) + x * mask\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_autoencoder(model, dataloader, criterion, optimizer, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for degraded, mask, gt in dataloader:\n",
    "            degraded, mask, gt = degraded.to(device), mask.to(device), gt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(degraded, mask)\n",
    "            loss = criterion(outputs, gt)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Transformation with resizing to 256x256\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize images to 256x256\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    degraded_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Train\\Degraded_image\\broken_small\"\n",
    "    mask_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Train\\Defect_mask\\broken_small\"\n",
    "    gt_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Train\\GT_clean_image\\broken_small\"\n",
    "    \n",
    "    dataset = DenoisingDataset(degraded_dir, mask_dir, gt_dir, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    model = MaskedAutoencoder() \n",
    "    criterion = nn.MSELoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model_trained = train_autoencoder(model, dataloader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Image transformation\u001b[39;00m\n\u001b[0;32m     66\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     67\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     68\u001b[0m ])\n\u001b[1;32m---> 71\u001b[0m \u001b[43msave_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36msave_predictions\u001b[1;34m(model, device, input_dir, output_dir, transform)\u001b[0m\n\u001b[0;32m     52\u001b[0m degraded_img \u001b[38;5;241m=\u001b[39m degraded_img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     53\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(degraded_img)  \u001b[38;5;66;03m# assuming no mask provided in val\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdegraded_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Save output image\u001b[39;00m\n\u001b[0;32m     57\u001b[0m output_img \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToPILImage()(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mMaskedAutoencoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     63\u001b[0m encoded_unmasked_upsampled \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[0;32m     64\u001b[0m     encoded_unmasked, size\u001b[38;5;241m=\u001b[39m(masked_input\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), masked_input\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     67\u001b[0m combined_decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((encoded_unmasked_upsampled, masked_input), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_decoder_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m decoded \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(decoded, size\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     72\u001b[0m output \u001b[38;5;241m=\u001b[39m decoded \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m+\u001b[39m x \u001b[38;5;241m*\u001b[39m mask\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py:953\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    948\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    949\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    951\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, degraded_dir, transform=None):\n",
    "        self.degraded_dir = degraded_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(degraded_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        degraded_img = Image.open(os.path.join(self.degraded_dir, img_name)).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            degraded_img = self.transform(degraded_img)\n",
    "        \n",
    "        return degraded_img, img_name\n",
    "\n",
    "\n",
    "def save_predictions(model, device, input_dir, output_dir, transform):\n",
    "    # Ensure the output directory has the same structure as the input directory\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    shutil.copytree(input_dir, output_dir, ignore=shutil.ignore_patterns(\"*.png\"))\n",
    "\n",
    "    # Iterate over objects in the dataset (e.g., 'bottle', 'cable')\n",
    "    for object_type in os.listdir(input_dir):\n",
    "        object_path = os.path.join(input_dir, object_type, \"Val\", \"Degraded_image\")\n",
    "        \n",
    "        # Iterate over defect types (e.g., 'broken_large', 'broken_small')\n",
    "        for defect_type in os.listdir(object_path):\n",
    "            defect_path = os.path.join(object_path, defect_type)\n",
    "\n",
    "            # Create a DataLoader for the current defect type\n",
    "            dataset = InferenceDataset(defect_path, transform)\n",
    "            dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "            # Directory to save predictions\n",
    "            output_defect_path = os.path.join(output_dir, object_type, \"Val\", \"Degraded_image\", defect_type)\n",
    "\n",
    "            # Run inference\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for degraded_img, img_name in dataloader:\n",
    "                    degraded_img = degraded_img.to(device)\n",
    "                    mask = torch.zeros_like(degraded_img)  # assuming no mask provided in val\n",
    "                    output = model(degraded_img, mask)\n",
    "\n",
    "                    # Save output image\n",
    "                    output_img = transforms.ToPILImage()(output.squeeze(0).cpu())\n",
    "                    output_img.save(os.path.join(output_defect_path, img_name[0]))\n",
    "\n",
    "# Parameters\n",
    "input_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\"\n",
    "output_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_results\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "save_predictions(model, device, input_dir, output_dir, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "val_dir = \"F:/KLA Problem Statement/Denoising_Dataset_train_val\"\n",
    "results_dir = \"F:/KLA Problem Statement/Denoising_Dataset_results\"\n",
    "\n",
    "Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def process_and_save_images(model, val_dir, results_dir, transform):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        for obj in os.listdir(val_dir):\n",
    "            obj_dir = os.path.join(val_dir, obj, 'Val', 'Degraded_image')\n",
    "            if not os.path.isdir(obj_dir):\n",
    "                continue\n",
    "            \n",
    "            for defect_type in os.listdir(obj_dir):\n",
    "                defect_dir = os.path.join(obj_dir, defect_type)\n",
    "                save_defect_dir = os.path.join(results_dir, obj, 'Val', defect_type)\n",
    "                Path(save_defect_dir).mkdir(parents=True, exist_ok=True)\n",
    "                for img_name in os.listdir(defect_dir):\n",
    "                    if img_name.endswith('.png'):\n",
    "                        degraded_img_path = os.path.join(defect_dir, img_name)\n",
    "                        mask_img_path = os.path.join(\n",
    "                            val_dir, obj, 'Val', 'Defect_mask', defect_type, img_name.replace('.png', '_mask.png')\n",
    "                        )\n",
    "\n",
    "                        degraded_img = Image.open(degraded_img_path).convert('RGB')\n",
    "                        mask_img = Image.open(mask_img_path).convert('L')\n",
    "                        degraded_img = transform(degraded_img).to(device)\n",
    "                        mask_img = transform(mask_img).to(device)\n",
    "                        mask_img = mask_img.unsqueeze(0)  \n",
    "                        output = model(degraded_img.unsqueeze(0), mask_img).squeeze(0)\n",
    "                        output_path = os.path.join(save_defect_dir, img_name)\n",
    "                        save_image(output, output_path)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "process_and_save_images(model_trained, val_dir, results_dir, transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate PSNR using torchmetrics\n",
    "psnr_metric = torchmetrics.functional.peak_signal_noise_ratio\n",
    "\n",
    "def calculate_psnr_ssim(output, gt):\n",
    "    \"\"\"Calculate PSNR and SSIM between the output and ground truth.\"\"\"\n",
    "    # Convert tensors to NumPy arrays for SSIM calculation\n",
    "    output_np = output.permute(0, 2, 3, 1).cpu().numpy()  # Convert to (N, H, W, C)\n",
    "    gt_np = gt.permute(0, 2, 3, 1).cpu().numpy()  # Convert to (N, H, W, C)\n",
    "\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for i in range(len(output_np)):\n",
    "        # PSNR\n",
    "        psnr_value = psnr_metric(output[i], gt[i], data_range=1.0)  # Normalized to [0,1]\n",
    "        psnr_list.append(psnr_value.item())\n",
    "        \n",
    "        # SSIM: Set `win_size` to a smaller value to ensure it fits within the image dimensions\n",
    "        ssim_value = ssim(gt_np[i], output_np[i], multichannel=True, data_range=1.0, win_size=3)\n",
    "        ssim_list.append(ssim_value)\n",
    "\n",
    "    return psnr_list, ssim_list\n",
    "\n",
    "\n",
    "def visualize_results_with_metrics(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for degraded, mask, gt in dataloader:\n",
    "            # Move data to the device (GPU or CPU)\n",
    "            degraded, mask, gt = degraded.to(device), mask.to(device), gt.to(device)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(degraded, mask)\n",
    "            \n",
    "            # Calculate PSNR and SSIM\n",
    "            psnr_list, ssim_list = calculate_psnr_ssim(output, gt)\n",
    "            \n",
    "            # Convert torch tensors to numpy arrays for plotting\n",
    "            degraded_np = degraded.cpu().numpy()\n",
    "            mask_np = mask.cpu().numpy()\n",
    "            gt_np = gt.cpu().numpy()\n",
    "            output_np = output.cpu().numpy()\n",
    "            \n",
    "            # Plot the first few images in the batch\n",
    "            for i in range(min(len(degraded_np), 5)):  # Visualize up to 5 images\n",
    "                fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "                \n",
    "                axs[0].imshow(degraded_np[i].transpose(1, 2, 0))  # Degraded image\n",
    "                axs[0].set_title('Degraded Image')\n",
    "                axs[0].axis('off')\n",
    "                \n",
    "                axs[1].imshow(mask_np[i].squeeze(), cmap='gray')  # Mask image\n",
    "                axs[1].set_title('Mask Image')\n",
    "                axs[1].axis('off')\n",
    "                \n",
    "                axs[2].imshow(gt_np[i].transpose(1, 2, 0))  # Ground Truth image\n",
    "                axs[2].set_title('Ground Truth')\n",
    "                axs[2].axis('off')\n",
    "                \n",
    "                axs[3].imshow(output_np[i].transpose(1, 2, 0))  # Model Output\n",
    "                axs[3].set_title(f'Reconstructed Image\\nPSNR: {psnr_list[i]:.2f} SSIM: {ssim_list[i]:.2f}')\n",
    "                axs[3].axis('off')\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "# Directories for test dataset\n",
    "test_degraded_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\\\bottle\\Val\\Degraded_image\\broken_small\"\n",
    "test_mask_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Val\\Defect_mask\\broken_small\"\n",
    "test_gt_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Val\\GT_clean_image\\broken_small\"\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = DenoisingDataset(test_degraded_dir, test_mask_dir, test_gt_dir, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Visualize results with PSNR and SSIM\n",
    "visualize_results_with_metrics(model, test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate PSNR using torchmetrics\n",
    "psnr_metric = torchmetrics.functional.peak_signal_noise_ratio\n",
    "\n",
    "def calculate_psnr_ssim(output, gt):\n",
    "    \"\"\"Calculate PSNR and SSIM between the output and ground truth.\"\"\"\n",
    "    # Convert tensors to NumPy arrays for SSIM calculation\n",
    "    output_np = output.permute(0, 2, 3, 1).cpu().numpy()  # Convert to (N, H, W, C)\n",
    "    gt_np = gt.permute(0, 2, 3, 1).cpu().numpy()  # Convert to (N, H, W, C)\n",
    "\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for i in range(len(output_np)):\n",
    "        # PSNR\n",
    "        psnr_value = psnr_metric(output[i], gt[i], data_range=1.0)  # Normalized to [0,1]\n",
    "        psnr_list.append(psnr_value.item())\n",
    "        \n",
    "        # SSIM: Set `win_size` to a smaller value to ensure it fits within the image dimensions\n",
    "        ssim_value = ssim(gt_np[i], output_np[i], multichannel=True, data_range=1.0, win_size=3)\n",
    "        ssim_list.append(ssim_value)\n",
    "\n",
    "    return psnr_list, ssim_list\n",
    "\n",
    "def save_results_with_metrics(model, dataloader, device, save_dir):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (degraded, mask, gt) in enumerate(dataloader):\n",
    "            # Move data to the device (GPU or CPU)\n",
    "            degraded, mask, gt = degraded.to(device), mask.to(device), gt.to(device)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            output = model(degraded, mask)\n",
    "            \n",
    "            # Calculate PSNR and SSIM\n",
    "            psnr_list, ssim_list = calculate_psnr_ssim(output, gt)\n",
    "            \n",
    "            # Convert torch tensors to numpy arrays for saving\n",
    "            degraded_np = degraded.cpu().numpy()\n",
    "            mask_np = mask.cpu().numpy()\n",
    "            gt_np = gt.cpu().numpy()\n",
    "            output_np = output.cpu().numpy()\n",
    "            \n",
    "            # Save the first few images in the batch\n",
    "            for i in range(len(degraded_np)):  # Save all images in the batch\n",
    "                fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "                \n",
    "                axs[0].imshow(degraded_np[i].transpose(1, 2, 0))  # Degraded image\n",
    "                axs[0].set_title('Degraded Image')\n",
    "                axs[0].axis('off')\n",
    "                \n",
    "                axs[1].imshow(mask_np[i].squeeze(), cmap='gray')  # Mask image\n",
    "                axs[1].set_title('Mask Image')\n",
    "                axs[1].axis('off')\n",
    "                \n",
    "                axs[2].imshow(gt_np[i].transpose(1, 2, 0))  # Ground Truth image\n",
    "                axs[2].set_title('Ground Truth')\n",
    "                axs[2].axis('off')\n",
    "                \n",
    "                axs[3].imshow(output_np[i].transpose(1, 2, 0))  # Model Output\n",
    "                axs[3].set_title(f'Reconstructed Image\\nPSNR: {psnr_list[i]:.2f} SSIM: {ssim_list[i]:.2f}')\n",
    "                axs[3].axis('off')\n",
    "                \n",
    "                # Save the figure\n",
    "                save_path = os.path.join(save_dir, f'batch_{batch_idx}_image_{i}.png')\n",
    "                plt.savefig(save_path)\n",
    "                plt.close(fig)  # Close the figure to avoid memory leaks\n",
    "\n",
    "# Directories for test dataset\n",
    "test_degraded_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\\\bottle\\Val\\Degraded_image\\broken_small\"\n",
    "test_mask_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Val\\Defect_mask\\broken_small\"\n",
    "test_gt_dir = r\"F:\\KLA Problem Statement\\Denoising_Dataset_train_val\\bottle\\Val\\GT_clean_image\\broken_small\"\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = DenoisingDataset(test_degraded_dir, test_mask_dir, test_gt_dir, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Directory to save images\n",
    "save_dir = r\"F:\\KLA Problem Statement\\Saved_Results\"\n",
    "\n",
    "# Save the results with PSNR and SSIM metrics\n",
    "save_results_with_metrics(model, test_dataloader, device, save_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
